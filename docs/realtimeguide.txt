Real-time capabilities in AI applications are no longer a luxury — they are a necessity. Whether live chatbots, instant text generation, real-time translation or responsive gaming assistants, the demand for instantaneous AI-powered interactions has skyrocketed. OpenAI’s Realtime API provides a robust framework to create such dynamic experiences, blending the power of large language models (LLMs) with real-time responsiveness.
This tutorial will explore building AI applications using OpenAI’s Realtime API. It will provide everything you need to start, including setting up your environment and crafting advanced real-time applications.
What Is OpenAI’s Realtime API?
OpenAI’s Realtime API is designed for applications requiring low-latency responses from powerful language models like GPT-4. It supports streaming responses, making it ideal for use cases such as:
    • Interactive chatbots 
    • Live collaborative tools 
    • Real-time content generation 
    • On-the-fly translation 
The API bridges the gap between cloud-based AI capabilities and the immediacy required in real-world applications by enabling faster, more dynamic interactions.
Prerequisites
Before diving into this tutorial, ensure you have the following:
 
Andela provides the world’s largest private marketplace for global remote tech talent driven by an AI-powered platform to manage the complete contract hiring lifecycle. Andela helps companies scale teams & deliver projects faster via specialized areas: App Engineering, AI, Cloud, Data & Analytics. 
Learn More
The latest from Andela 
Andela | Why Should Python Developers Care About Testing
20 December 2024
Andela | Mastering Progressive Hydration for Enhanced Web Performance
19 December 2024
Andela | Insights and Trends from Recent Tech Events: What You Need to Know
18 December 2024
    1. Basic knowledge of Python programming. 
    2. An OpenAI API key. If you don’t have one, sign up at OpenAI’s platform. 
    3. Python 3.7+ installed on your machine. 
Install the required libraries:
pip install openai asyncio websockets
TRENDING STORIES 
    1. The Hidden Cost of Unused Code 
    2. How K8s CPU Requests and Limits Actually Work — Chapter 2 
    3. Mastering OpenAI’s Realtime API: A Comprehensive Guide 
    4. AWS Discontinues Git-Hosting Service CodeCommit 
    5. PlatformCon: How Spotify Manages Infrastructure with GitOps 
Key Features of the Realtime API
    1. Streaming responses: The API streams responses token by token, enabling real-time updates in user interfaces. 
    2. Low latency: Optimized infrastructure ensures minimal response delay. 
    3. Scalability: Supports high-concurrency applications for large-scale deployments. 
    4. Fine-grained control: Allows developers to manage token limits, streaming configurations and model behaviors. 
Step 1: Setting Up Your Environment
To start, import the necessary libraries and set your OpenAI API key. This key authenticates your application and provides access to the API.
import openai import asyncio # Set your OpenAI API key openai.api_key = "your_openai_api_key"
1
2
3
4
5
import openai
import asyncio
 
# Set your OpenAI API key
openai.api_key = "your_openai_api_key"


Ensure your API key is stored securely. Avoid hardcoding it in production environments. Use environment variables or secure vaults like AWS Secrets Manager.
Step 2: Basic Realtime API Usage
Let’s create a simple script that streams responses from GPT-4 to understand how the Realtime API works.
Import open ai Async def stream_response(prompt): response = openai.ChatCompletion.create( model="gpt-4", messages=[{"role": "user", "content": prompt}], stream=True #Enable streaming ) print ("Response:") async for message in response: print (message.choices[0].delta.get ("content", ""), end="", flush=True) #Example prompt Asyncio.run(stream_response ("Explain the significance of the Eiffel Tower."))
1
2
3
4
5
6
7
8
9
10
11
12
Import open ai
Async def stream_response(prompt):
      response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True  #Enable streaming 
     )
      print ("Response:")
      async for message in response:
            print (message.choices[0].delta.get ("content", ""), end="", flush=True)
#Example prompt
Asyncio.run(stream_response ("Explain the significance of the Eiffel Tower."))

Key Points:
    • Stream=True: Enables streaming responses. 
    • Delta: The delta field in the API response contains new tokens generated by the model. 
Step 3: Building a Real-Time Chatbot
A chatbot is one of the most common real-time AI applications. Let’s build a bot that interacts with users and streams responses dynamically.
Implementation
import openai import asyncio async def real_time_chat(): print("Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)") while True: user_input = input("You: ") if user_input.lower() == "exit": print("Chatbot: Goodbye!") break print("Chatbot: ", end="", flush=True) response = openai.ChatCompletion.create( model="gpt-4", messages=[{"role": "user", "content": user_input}], stream=True ) async for message in response: print(message.choices[0].delta.get("content", ""), end="", flush=True) print() # Run the chatbot asyncio.run(real_time_chat())
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
import openai
import asyncio
 
async def real_time_chat():
print("Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)")
while True:
user_input = input("You: ")
if user_input.lower() == "exit":
print("Chatbot: Goodbye!")
break
 
print("Chatbot: ", end="", flush=True)
 
response = openai.ChatCompletion.create(
model="gpt-4",
messages=[{"role": "user", "content": user_input}],
stream=True
)
 
async for message in response:
print(message.choices[0].delta.get("content", ""), end="", flush=True)
print()
 
# Run the chatbot
asyncio.run(real_time_chat())


This chatbot streams responses in real time, creating a seamless conversational experience.
Step 4: Adding Features to the Chatbot
To make the chatbot more functional, let’s add:
    1. Context retention: Keep track of previous messages to provide meaningful, context-aware replies. 
    2. Error handling: Handle API rate limits and other errors gracefully. 
Enhanced Chatbot Code
import openai import asyncio async def enhanced_real_time_chat(): conversation_history = [] # Store previous messages print("Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)") while True: user_input = input("You: ") if user_input.lower() == "exit": print("Chatbot: Goodbye!") break # Append user input to conversation history conversation_history.append({"role": "user", "content": user_input}) try: print("Chatbot: ", end="", flush=True) response = openai.ChatCompletion.create( model="gpt-4", messages=conversation_history, stream=True ) async for message in response: content = message.choices[0].delta.get("content", "") print(content, end="", flush=True) print() # Append model's response to conversation history conversation_history.append({"role": "assistant", "content": content}) except openai.error.RateLimitError: print("Chatbot: Sorry, I'm currently overloaded. Please try again later.") except Exception as e: print(f"Chatbot: An error occurred: {e}") # Run the enhanced chatbot asyncio.run(enhanced_real_time_chat())
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
import openai
import asyncio
 
async def enhanced_real_time_chat():
conversation_history = [] # Store previous messages
 
print("Chatbot: Hello! How can I assist you today? (Type 'exit' to quit)")
while True:
user_input = input("You: ")
if user_input.lower() == "exit":
print("Chatbot: Goodbye!")
break
 
# Append user input to conversation history
conversation_history.append({"role": "user", "content": user_input})
 
try:
print("Chatbot: ", end="", flush=True)
response = openai.ChatCompletion.create(
model="gpt-4",
messages=conversation_history,
stream=True
)
 
async for message in response:
content = message.choices[0].delta.get("content", "")
print(content, end="", flush=True)
 
print()
# Append model's response to conversation history
conversation_history.append({"role": "assistant", "content": content})
 
except openai.error.RateLimitError:
print("Chatbot: Sorry, I'm currently overloaded. Please try again later.")
except Exception as e:
print(f"Chatbot: An error occurred: {e}")
 
# Run the enhanced chatbot
asyncio.run(enhanced_real_time_chat())

Step 5: Advanced Applications
Real-Time Collaboration Tool
Imagine a real-time collaborative tool where multiple users can generate content simultaneously. The Realtime API makes this possible by supporting concurrent requests.
import openai import asyncio async def collaborative_tool(prompts): tasks = [] for prompt in prompts: tasks.append(asyncio.create_task(stream_response(prompt))) await asyncio.gather(*tasks) # Example prompts for collaboration prompts = [ "Draft an email about project updates.", "Create a motivational quote for a presentation.", "Generate a summary of the latest AI trends." ] # Run the collaborative tool asyncio.run(collaborative_tool(prompts))
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
import openai
import asyncio
 
async def collaborative_tool(prompts):
tasks = []
for prompt in prompts:
tasks.append(asyncio.create_task(stream_response(prompt)))
await asyncio.gather(*tasks)
 
# Example prompts for collaboration
prompts = [
"Draft an email about project updates.",
"Create a motivational quote for a presentation.",
"Generate a summary of the latest AI trends."
]
 
# Run the collaborative tool
asyncio.run(collaborative_tool(prompts))

Step 6: Real-Time Translation API
OpenAI’s Realtime API can also power live translation services. Let’s build a simple translator.
async def real_time_translator(text, target_language): prompt = f"Translate this text to {target_language}: {text}" await stream_response(prompt) # Example usage asyncio.run(real_time_translator("Hello, how are you?", "French"))
1
2
3
4
5
6
async def real_time_translator(text, target_language):
prompt = f"Translate this text to {target_language}: {text}"
await stream_response(prompt)
 
# Example usage
asyncio.run(real_time_translator("Hello, how are you?", "French"))


This implementation dynamically streams translations, which is ideal for live communication tools.
Step 7: Optimizing Real-Time Performance
    1. Batching requests: For applications handling high traffic, batch similar requests to optimize API calls. 
    2. Token limits: Set token limits to manage response size and reduce latency. 
    3. Caching responses: Use caching mechanisms for repeated queries to minimize API usage. 
Step 8: Deploying Real-Time Applications
Deploying your application involves:
    • Backend deployment: Use frameworks like FastAPI or Flask to serve your real-time application. 
    • Frontend integration: Use WebSockets for real-time updates in web applications. 
    • Monitoring: Implement logging and monitoring to track API usage and performance. 

